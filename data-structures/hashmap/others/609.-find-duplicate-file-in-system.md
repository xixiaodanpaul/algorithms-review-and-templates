# 609. Find Duplicate File in System

## Question

Given a list `paths` of directory info, including the directory path, and all the files with contents in this directory, return _all the duplicate files in the file system in terms of their paths_. You may return the answer in **any order**.

A group of duplicate files consists of at least two files that have the same content.

A single directory info string in the input list has the following format:

* `"root/d1/d2/.../dm f1.txt(f1_content) f2.txt(f2_content) ... fn.txt(fn_content)"`

It means there are `n` files `(f1.txt, f2.txt ... fn.txt)` with content `(f1_content, f2_content ... fn_content)` respectively in the directory "`root/d1/d2/.../dm"`. Note that `n >= 1` and `m >= 0`. If `m = 0`, it means the directory is just the root directory.

The output is a list of groups of duplicate file paths. For each group, it contains all the file paths of the files that have the same content. A file path is a string that has the following format:

* `"directory_path/file_name.txt"`

**Example 1:**

```text
Input: paths = ["root/a 1.txt(abcd) 2.txt(efgh)","root/c 3.txt(abcd)","root/c/d 4.txt(efgh)","root 4.txt(efgh)"]
Output: [["root/a/2.txt","root/c/d/4.txt","root/4.txt"],["root/a/1.txt","root/c/3.txt"]]
```

**Example 2:**

```text
Input: paths = ["root/a 1.txt(abcd) 2.txt(efgh)","root/c 3.txt(abcd)","root/c/d 4.txt(efgh)"]
Output: [["root/a/2.txt","root/c/d/4.txt"],["root/a/1.txt","root/c/3.txt"]]
```

**Constraints:**

* `1 <= paths.length <= 2 * 10^4`
* `1 <= paths[i].length <= 3000`
* `1 <= sum(paths[i].length) <= 5 * 10^5`
* `paths[i]` consist of English letters, digits, `'/'`, `'.'`, `'('`, `')'`, and `' '`.
* You may assume no files or directories share the same name in the same directory.
* You may assume each given directory info represents a unique directory. A single blank space separates the directory path and file info.

**Follow up:**

* Imagine you are given a real file system, how will you search files? DFS or BFS?
* If the file content is very large \(GB level\), how will you modify your solution?
* If you can only read the file by 1kb each time, how will you modify your solution?
* What is the time complexity of your modified solution? What is the most time-consuming part and memory-consuming part of it? How to optimize?
* How to make sure the duplicated files you find are not false positive?

## Solution

```cpp
class Solution {
public:
    vector<vector<string>> findDuplicate(vector<string>& paths) {
        // sol: hashmap
        vector<vector<string>> res;
        unordered_map<string, vector<string>> m;
        for (auto& path : paths) {
            istringstream in(path);
            string prefix;
            string file;
            in >> prefix;
            while (in >> file) {
                int idx = file.find_last_of('(');
                string filePath = prefix + "/" + file.substr(0, idx);
                string content = file.substr(idx+1, file.size()-idx-2);
                m[content].push_back(filePath);
            }
        }
        for (auto& it : m) {
            if (it.second.size() > 1) {
                res.push_back(it.second);
            }
        }
        return res;
    }
};
```

**Follow up:**

{% tabs %}
{% tab title="Solution1" %}
```cpp
BFS vs DFS
BFS explores neighbors first. This means that files which are located close to each other are also accessed one after another. This is great for space locality and that's why BFS is expected to be faster. Also, BFS is easier to parallelize (more fine-grained locking). DFS will require a lock on the root node.
Very large files and false positives
For very large files we should do the following comparisons in this order:
compare sizes, if not equal, then files are different and stop here!
hash them with a fast algorithm e.g. MD5 or use SHA256 (no collisions found yet), if not equal then stop here!
compare byte by byte to avoid false positives due to collisions.
Have you used an IDE in remote development mode?
For example, CLion has some options on how to compare the local files with the remote server files and then decides to synchronize or not.
Complexity
Runtime - Worst case (which is very unlikely to happen): O(N^2 * L) where L is the size of the maximum bytes that need to be compared
Space - Worst case: all files are hashed and inserted in the hashmap, so O(H^2*L), H is the hash code size and L is the filename size
```
{% endtab %}

{% tab title="Solution2" %}
```
1. Imagine you are given a real file system, how will you search files? DFS or BFS?
DFS. In this case the directory path could be large. DFS can reuse the shared the parent directory before leaving that directory. But BFS cannot.

2. If the file content is very large (GB level), how will you modify your solution?
In this case, not realistic to match the whole string of the content. So we use file signitures to judge if two files are identical. Signitures can include file size, as well as sampled contents on the same positions. They could have different file names and time stamps though.
Hashmaps are necessary to store the previous scanned file info. S = O(|keys| + |list(directory)|). The key and the directory strings are the main space consumption.
a. Sample to obtain the sliced indices in the strings stored in the RAM only once and used for all the scanned files. Accessing the strings is on-the-fly. But transforming them to hashcode used to look up in hashmap and storing the keys and the directories in the hashmap can be time consuming. The directory string can be compressed to directory id. The keys are hard to compress.
b. Use fast hashing algorithm e.g. MD5 or use SHA256 (no collisions found yet). If no worry about the collision, meaning the hashcode is 1-1. Thus in the hashmap, the storage consumption on key string can be replaced by key_hashcode, space usage compressed.

3. If you can only read the file by 1kb each time, how will you modify your solution?
That is the file cannot fit the whole ram. Use a buffer to read controlled by a loop; read until not needed or to the end. The sampled slices are offset by the times the buffer is called.

4. What is the time complexity of your modified solution? What is the most time-consuming part and memory consuming part of it? How to optimize?
T = O(|num_files||sample||directory_depth|) + O(|hashmap.keys()|)

5. How to make sure the duplicated files you find are not false positive?
Add a round of final check which checks the whole string of the content. T = O(|num_output_list||max_list_size||file_size|).
```
{% endtab %}
{% endtabs %}

#### 

